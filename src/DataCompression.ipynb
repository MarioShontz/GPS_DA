{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbe7469-1957-4ee0-80c8-7fe4c3d0bc5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgumentParser\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pl\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(1234)\n",
    "from torch import nn\n",
    "import torch\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule\n",
    "\n",
    "from image_plotting_callback import ImageSampler\n",
    "\n",
    "\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "            latent_dim=latent_dim,\n",
    "            input_height=input_height,\n",
    "            first_conv=False,\n",
    "            maxpool1=False\n",
    "        )\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--gpus', type=int, default=None)\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        dataset = CIFAR10DataModule('.')\n",
    "    if args.dataset == 'imagenet':\n",
    "        dataset = ImagenetDataModule('.')\n",
    "\n",
    "    sampler = ImageSampler()\n",
    "\n",
    "    vae = VAE()\n",
    "    trainer = pl.Trainer(gpus=args.gpus, max_epochs=20, callbacks=[sampler])\n",
    "    trainer.fit(vae, dataset)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189ff534-9c1e-4ab5-9339-5a0630c08724",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pl_bolts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpl_bolts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VAE\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE()\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pl_bolts'"
     ]
    }
   ],
   "source": [
    "from pl_bolts.models.autoencoders import VAE\n",
    "\n",
    "model = VAE()\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222544e6-f10b-4134-8da7-6072f3d33ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
